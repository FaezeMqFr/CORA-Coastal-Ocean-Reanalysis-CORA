{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad43a2c-71af-4a9f-8abe-85d13c144a14",
   "metadata": {
    "id": "8d6fc216-c8ff-4124-94f1-c8284f25f8f0"
   },
   "source": [
    "# This notebook allows users to upload a .csv file of extracted CORA time series and run it through NOAAâ€™s Tidal Analysis Datum Calculator (TADC) to convert data from Mean Sea Level (MSL) to other Datums. To run this notebook it will be necessary to also have the Python script and config file for the calculator, which are available on the GitHub repository: https://github.com/NOAA-CO-OPS/CO-OPS-Tidal-Analysis-Datum-Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d869be17-baf2-4fc3-91ef-561e331818fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import detrend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5efc3d-62de-4be1-a34d-76367b33ec45",
   "metadata": {},
   "source": [
    "**Use glob to loop through your CORA .csv files and run the data through the TADC using subprocess with SDC.py.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8452c078-b947-447d-b2ac-0df89d4195a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the folder containing the CSV files and use glob to iterate through the files\n",
    "dirname = 'path\\\\to\\\\csvfiles\\\\'\n",
    "csv_path = dirname + '*.csv'\n",
    "configpath = 'path\\\\to\\\\TADC_config_file_directory\\\\'\n",
    "\n",
    "i=0\n",
    "for fname in natsorted(glob.glob(csv_path)): # natsorted makes sure of the natural sorting of files\n",
    "    \n",
    "    file_name = os.path.split(fname)[-1]  # Extract the filename from the path\n",
    "    \n",
    "    with open(configpath + 'config.cfg', \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "        lines[22] = 'fname = ' + dirname + file_name + '\\n'\n",
    "        lines[47] = 'subordinate_lon = ' + str(stations_df.lng.iloc[i]) + '\\n'\n",
    "        lines[51] = 'subordinate_lat = ' + str(stations_df.lat.iloc[i]) + '\\n'\n",
    "    \n",
    "    with open(configpath + 'config.cfg', \"w\") as file:\n",
    "        file.writelines(lines)\n",
    "        \n",
    "    proc = subprocess.run(['python', configpath + 'SDC.py'], capture_output=True, text=True)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0558b8-a821-4b05-8c0e-2a5059bb9ce8",
   "metadata": {},
   "source": [
    "**Specify the path to the folder containing the CSV files and use glob to iterate through the files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f4682cf-ac94-4091-84e4-0e81557c2de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datastart should match the start date of your CORA water level data file\n",
    "datastart = \"Data Start:  2018-09-01 00:00:00\"\n",
    "folder_path = dirname + '*.out'\n",
    "\n",
    "fulldata = []\n",
    "\n",
    "for fname in natsorted(glob.glob(folder_path)): # natsorted makes sure of the natural sorting of files\n",
    "\n",
    "    file_name = os.path.split(fname)[-1]  # Extract the filename from the path\n",
    "    out_path = dirname + file_name\n",
    "\n",
    "    with open(out_path, \"r\") as file:\n",
    "\n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            if datastart in line:\n",
    "                fulldata.append(int(file_name[0:7])) # these are the points that included a full 19 years of data\n",
    "                break  # Stop after finding the first match\n",
    "\n",
    "# for i in range(len(fulldata)):\n",
    "\n",
    "    # head_tail= os.path.split(str(fulldata[i]))\n",
    "    head_tail= os.path.split(fname)\n",
    "    \n",
    "    file_name = head_tail[1]  # Extract the filename from the path\n",
    "    out_path = dirname + file_name\n",
    "    # file_stem = Path(head_tail[1]).stem\n",
    "    \n",
    "    with open(out_path, \"r\") as file:\n",
    "        \n",
    "        lines = len(file.readlines())\n",
    "        \n",
    "    with open(out_path, \"r\") as file:\n",
    "        \n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            \n",
    "            if \"HWL\" in line:                   \n",
    "                    \n",
    "                colspecs = [(0,5),(7,15)]\n",
    "                df_fwf = pd.read_fwf(out_path, colspecs=colspecs,skiprows=line_number - 1, skipfooter=lines - line_number - 12, names=['Datums','meters'])\n",
    "                                \n",
    "                grid_fname = dirname + 'Datums_' + file_name + '.csv'\n",
    "                df_fwf.to_csv(grid_fname,index=False) # write the files to .csv\n",
    "\n",
    "                break  # Stop after finding the first match"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMAC_python",
   "language": "python",
   "name": "dmac_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
