{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad43a2c-71af-4a9f-8abe-85d13c144a14",
   "metadata": {
    "id": "8d6fc216-c8ff-4124-94f1-c8284f25f8f0"
   },
   "source": [
    "## The NOAA Coastal Ocean Reanalysis (CORA) dataset outputs are referenced to mean sea level. This notebook allows users to download CORA data from the NOAA Open Data Dissemination (NODD), save the data as a .csv file and then use the file(s) with NOAAâ€™s Tidal Analysis Datum Calculator (TADC) python script to convert data to other tidal datums. If you already have .csv files available, you can skip to the last couple of cell blocks of the script to the part where the TADC script is used with your files.\n",
    "***\n",
    "## **The TADC script (SDC.py) and it's associated config.cfg file are provided in the TADC_Files folder. A sample file is provided in the Sample_Files folder.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1789b88-df6a-406e-928f-e73b80009b3c",
   "metadata": {},
   "source": [
    "### First, import any necessary python modules. If a module is not found, you will need to pip (or conda) install the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d869be17-baf2-4fc3-91ef-561e331818fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dask\n",
    "import intake\n",
    "import intake_xarray\n",
    "import s3fs\n",
    "import holoviews as hv\n",
    "import geoviews as gv\n",
    "import hvplot.xarray\n",
    "import holoviews.operation.datashader as dshade\n",
    "from bokeh.models import DatetimeTickFormatter, HoverTool\n",
    "import cmocean\n",
    "from bokeh.resources import INLINE\n",
    "import bokeh.io\n",
    "from bokeh import *\n",
    "bokeh.io.output_notebook(INLINE)\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a56ea-6d57-4aad-a65d-06cde040d551",
   "metadata": {},
   "source": [
    "# ***<span style='color:Blue'> If you are using files of previously acquired data and are not acquiring new CORA data, skip down to the last 2 cell blocks of code where you can use your data with the Tidal Analysis Datum Calculator script. If you are acquiring new CORA data, continue through the following cell blocks of code. </span>***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a13ab2f-5bad-4fe2-8f3e-70cb1b78372a",
   "metadata": {
    "id": "04ef0505-ce20-4f42-895d-d0f9dfab4e6f"
   },
   "source": [
    "### Access the data on the NODD and initialize the available CORA datasets.\n",
    "\n",
    "### *This accesses a .yml file located on the NODD that shows which CORA output files are available to import.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adda2bff-ded8-4d6b-80de-e741c10a7807",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 913,
     "status": "ok",
     "timestamp": 1736446979356,
     "user": {
      "displayName": "John Ratcliff",
      "userId": "16218560795575667576"
     },
     "user_tz": 300
    },
    "id": "09ee33db-10b7-4cd1-887d-7b0aaced1d04",
    "outputId": "c7b3492b-1726-40df-8e20-845f796aa5d6"
   },
   "outputs": [],
   "source": [
    "# @title This accesses a .yml file located on the NODD that shows which CORA output files are available to import.\n",
    "catalog = intake.open_catalog(\"s3://noaa-nos-cora-pds/CORA_V1.1_intake.yml\",storage_options={'anon':True})\n",
    "list(catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6e967b-4983-4028-82f8-1c7fb6909c20",
   "metadata": {
    "id": "de2ecccc-4203-41d4-8d98-4ca8369433f0"
   },
   "source": [
    "#### ***CORA-V1.1-fort.63:*** Hourly water levels-- ***'zeta'*** is the water elevation variable referenced to mean sea level\n",
    "#### ***CORA-V1.1-swan_DIR.63:*** Hourly mean wave direction-- ***'swan_DIR'*** is the mean wave direction variable\n",
    "#### ***CORA-V1.1-swan_TPS.63:*** Hourly peak wave periods-- ***'swan_TPS'*** is the peak wave period variable\n",
    "#### ***CORA-V1.1-swan_HS.63:*** Hourly significant wave heights-- ***'swan_HS'*** is the significant wave height variable\n",
    "#### ***CORA-V1.1-Grid:*** Hourly water levels interpolated from model nodes to 500-meter resolution coastal grid-- ***'zeta'*** is the water elevation variable <br>\n",
    ">#### All datasets denoted as **'-timeseries'** are optimized for pulling long time series (greater than a few days)\n",
    ">#### For up to a few days of data, use the regular dataset (not labeled **'-timeseries'** in the catalog description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d198d8d7-8420-47c0-a2e2-3c00e2b29d43",
   "metadata": {},
   "source": [
    "### Using the .to_dask() command with the water level dataset located in the catalog will create an xarray dataset. There is data at 1,813,443 model nodes spanning 385,704 hours (44 years, 1979-2022). The 'zeta' hourly water level variable is given in dimensions of time and node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a699c0ba-8c86-4ad0-a9b5-e5c187efc23e",
   "metadata": {
    "executionInfo": {
     "elapsed": 14086,
     "status": "ok",
     "timestamp": 1736446996449,
     "user": {
      "displayName": "John Ratcliff",
      "userId": "16218560795575667576"
     },
     "user_tz": 300
    },
    "id": "f51cd6d9-b6ba-4f8d-8eb9-db5f17ebda33"
   },
   "outputs": [],
   "source": [
    "ds = catalog[\"CORA-V1.1-fort.63-timeseries\"].to_dask()\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3c143-469d-4a81-ad71-1ea67a3660c1",
   "metadata": {
    "id": "33108bd1-fa7b-4c80-8453-712e48ff5235"
   },
   "source": [
    "### *Which points would you like to query?* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d00a278-4c01-41a1-8860-b9f515d71eff",
   "metadata": {},
   "source": [
    "### Create a pandas dataframe with locations where you would like to pull CORA data. You could bring in your own file of locations using pd.read_csv('yourfile.csv'). For this example, we are pulling in coordinates for NOAA NWLON stations using the CO-OPS API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f9651-ce64-406b-968d-868899de3a01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "wt_sKI4hvfqw",
    "outputId": "35825902-eec7-47e6-a36f-aee33138a246"
   },
   "outputs": [],
   "source": [
    "units= 'metric'\n",
    "\n",
    "station_type = 'waterlevels'\n",
    "\n",
    "server = 'https://api.tidesandcurrents.noaa.gov/mdapi/prod/webapi/stations/' # accessing NOAA CO-OPS api\n",
    "\n",
    "myurl = (server + '.json?type='+station_type+'&units='+units)\n",
    "\n",
    "urlResponse = requests.get(myurl)\n",
    "content=urlResponse.json()\n",
    "\n",
    "stations = content['stations']\n",
    "stations_df = pd.DataFrame(stations)\n",
    "\n",
    "stations_df = stations_df[['id','lat','lng']]\n",
    "station_id = ['8665530'] # Using NOAA NWLON stations\n",
    "\n",
    "stations_df = stations_df[stations_df['id'].isin(station_id)]\n",
    "\n",
    "stations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8ad011-5461-435d-be2b-092809760be4",
   "metadata": {},
   "source": [
    "### A function is created to find the nearest node to a given set of coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b5d5e-c4d2-44eb-8478-90f38358a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the indices of the points in (x,y) closest to the points in (xi,yi)\n",
    "def nearxy(x,y,xi,yi):\n",
    "    ind = np.ones(len(xi),dtype=int)\n",
    "    for i in range(len(xi)):\n",
    "        dist = np.sqrt((x-xi[i])**2+(y-yi[i])**2)\n",
    "        ind[i] = dist.argmin()\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f1e577-2f95-4739-9e46-4edc7dd96443",
   "metadata": {},
   "source": [
    "### Run the function in a loop to create a list of node indices where you want data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbdc3ed-622b-4f3e-8b05-5ba8d38737fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ds['x'].values\n",
    "y = ds['y'].values\n",
    "\n",
    "nodeidx = []\n",
    "for i in range(0,len(stations_df)):\n",
    "    ind = nearxy(x,y,[stations_df.lng.iloc[i]],[stations_df.lat.iloc[i]])\n",
    "    nodeidx.append(ind)\n",
    "\n",
    "nodeidx = [item for sublist in nodeidx for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaf7401-00d5-4a66-84ff-bd17f8bcfaf6",
   "metadata": {},
   "source": [
    "### Extract data for a specified time range, create dataframes for saving the data, then create plots using hvplot. Here, the hourly water levels are accessed for the specified node indices and time range. Using .compute() calls the dataset into memory, so this can take awhile depending on the length of data and number of nodes you are requesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6195b1-7f36-4b37-892e-58734a744633",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "start=\"1983-01-01 00:00:00\"\n",
    "end=\"2001-12-31 23:00:00\"\n",
    "\n",
    "zeta_tslice = ds['zeta'].sel(time=slice(start, end),node=nodeidx).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892c0554-271a-48da-bdde-288212f17dac",
   "metadata": {},
   "source": [
    "### Plot your time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d5ffb-aaea-4091-816f-6a28ab844a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "\n",
    "tickfmt = DatetimeTickFormatter(years=\"%m/%d/%Y\", months=\"%m/%d/%Y\", days = '%m/%d/%Y', hourmin = '%H:%M')\n",
    "tooltips = [\n",
    "    (\"time\", \"@time{%F %T}\"),\n",
    "    (\"water level\", \"@zeta\"),\n",
    "]\n",
    "hover = HoverTool(tooltips=tooltips,formatters={\n",
    "        '@time': 'datetime'})\n",
    "\n",
    "plots = []\n",
    "for i in range(0,len(stations_df)):\n",
    "\n",
    "    plot = zeta_tslice[:,i].hvplot(x='time', grid=True, xformatter=tickfmt, tools=[hover])\n",
    "\n",
    "    plots.append(plot)\n",
    "\n",
    "pn.Column(*plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d40fbd4-24ff-49bc-bc9f-ab237c699f3c",
   "metadata": {},
   "source": [
    "### Save the CORA data at each location as a .csv file. To save each file, the data that was acquired at each location is stored in a pandas dataframe and then written to a .csv file using pd.to_csv()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe94f60-3142-49e7-ae90-81fc891b1abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dt_range = pd.date_range(start, end, freq='h',inclusive='both')\n",
    "zeta_df = pd.DataFrame({'time': dt_range})\n",
    "\n",
    "filepath = 'WorkingDirectory\\\\Sample_Files\\\\' # Sample_Files is provided in the Github repository\n",
    "\n",
    "for i in range(0,len(stations_df)):\n",
    "    zeta_df['zeta'] = zeta_tslice[:,i]\n",
    "    zeta_df = zeta_df.round(3) # round the wl digits to 3 for use with the TAD calculator\n",
    "    zeta_df['time'] = pd.to_datetime(zeta_df['time'])\n",
    "    zeta_df['time'] = zeta_df['time'].dt.strftime('%m/%d/%Y %H:%M') # change the datetime format for use with the TAD calculator\n",
    "\n",
    "    wse_filename = str(stations_df.id.iloc[i]) + '_CORA' + '.csv' # create filename with station id and time range\n",
    "    zeta_df[['time',\"zeta\"]].to_csv(filepath+wse_filename,index=False) # write the files to .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f99c68-a784-4f68-bdca-be9b8e8f6783",
   "metadata": {},
   "source": [
    "# ***<span style='color:Blue'> Start here if you are using files of previously acquired data. Below, point to your file directory with the filepath variable. </span>***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5efc3d-62de-4be1-a34d-76367b33ec45",
   "metadata": {},
   "source": [
    "### Loop through your CORA .csv files and run the data through the TADC using subprocess with the SDC.py file in the TADC_Files folder. The config.cfg file is altered on each loop and used by SDC.py. Two output files will be written out for each file (the tidal datums .out file and a Highs/Lows .csv file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8452c078-b947-447d-b2ac-0df89d4195a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the folder containing the CSV files and use glob to iterate through the files\n",
    "configpath = 'WorkingDirectory\\\\TADC_Files\\\\' # TADC_Files is provided in the Github repository\n",
    "filepath = 'WorkingDirectory\\\\Sample_Files\\\\' # Sample_Files is provided in the Github repository\n",
    "csv_path = filepath + '*.csv'\n",
    "\n",
    "for fname in natsorted(glob.glob(csv_path)): # natsorted makes sure of the natural sorting of files\n",
    "    \n",
    "    file_name = os.path.split(fname)[-1]  # Extract the filename from the path\n",
    "    station = int(file_name[0:7])\n",
    "    with open(configpath + 'config.cfg', \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "        lines[22] = 'fname = ' + filepath + file_name + '\\n'\n",
    "        lines[47] = 'subordinate_lon = ' + str(station) + '\\n'\n",
    "        lines[51] = 'subordinate_lat = ' + str(station) + '\\n'\n",
    "    \n",
    "    with open(configpath + 'config.cfg', \"w\") as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "    with open(configpath + 'SDC.py', \"r\") as file:\n",
    "        SDClines = file.readlines()\n",
    "\n",
    "        SDClines[158] = 'CONFIG_FILE = ' + 'r\"' + configpath + 'config.cfg\"' + ' \\n'\n",
    "\n",
    "    with open(configpath + 'SDC.py', \"w\") as file:\n",
    "        file.writelines(SDClines)\n",
    "        \n",
    "    proc = subprocess.run(['python', configpath + 'SDC.py'], capture_output=True, text=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0558b8-a821-4b05-8c0e-2a5059bb9ce8",
   "metadata": {},
   "source": [
    "### This last part parses the tidal datums elevation table from the .out file and writes it to a new .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4682cf-ac94-4091-84e4-0e81557c2de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datastart should match the start date of your CORA water level data file; If using multiple locations, all need to start on same date for this code to work properly\n",
    "start=\"1983-01-01 00:00:00\" # all files that you are looping through should start at the same time\n",
    "datastart = \"Data Start:  \" + start\n",
    "outputpath = filepath + 'Outputs\\\\'\n",
    "folder_path = outputpath + '*.out'\n",
    "\n",
    "fulldata = []\n",
    "\n",
    "for fname in natsorted(glob.glob(folder_path)): # natsorted makes sure of the natural sorting of files\n",
    "\n",
    "    file_name = os.path.split(fname)[-1]  # Extract the filename from the path\n",
    "    out_path = outputpath + file_name\n",
    "\n",
    "    with open(out_path, \"r\") as file:\n",
    "\n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            if datastart in line:\n",
    "                fulldata.append(int(file_name[0:7])) \n",
    "                break  # Stop after finding the first match\n",
    "    \n",
    "    with open(out_path, \"r\") as file:\n",
    "        \n",
    "        lines = len(file.readlines())\n",
    "        \n",
    "    with open(out_path, \"r\") as file:\n",
    "        \n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            \n",
    "            if \"HWL\" in line:                   \n",
    "                    \n",
    "                colspecs = [(0,5),(7,15)]\n",
    "                df_fwf = pd.read_fwf(out_path, colspecs=colspecs,skiprows=line_number - 1, skipfooter=lines - line_number - 12, names=['Datums','meters'])\n",
    "                                \n",
    "                grid_fname = outputpath + 'Datums_' + file_name + '.csv'\n",
    "                df_fwf.to_csv(grid_fname,index=False) # write the files to .csv\n",
    "\n",
    "                break  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMAClean4",
   "language": "python",
   "name": "dmaclean4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
