{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad43a2c-71af-4a9f-8abe-85d13c144a14",
   "metadata": {
    "id": "8d6fc216-c8ff-4124-94f1-c8284f25f8f0"
   },
   "source": [
    "# This notebook pulls in prepared .csv files that contain CORA time series data and runs the data through the NOAA Tidal Analysis Datum Calculator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d869be17-baf2-4fc3-91ef-561e331818fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import detrend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9681b020-f6b1-44c6-8ff8-76a777aa33ff",
   "metadata": {},
   "source": [
    "**Create a dataframe of NWLON station ids and coordinates from the CO-OPS API where you want to use CORA data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af6b8168-ff60-449b-b632-b112862c7a0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "wt_sKI4hvfqw",
    "outputId": "35825902-eec7-47e6-a36f-aee33138a246"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>8651370</td>\n",
       "      <td>36.1833</td>\n",
       "      <td>-75.746696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>8656483</td>\n",
       "      <td>34.7175</td>\n",
       "      <td>-76.671111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id      lat        lng\n",
       "71  8651370  36.1833 -75.746696\n",
       "74  8656483  34.7175 -76.671111"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units= 'metric'\n",
    "\n",
    "station_type = 'waterlevels'\n",
    "\n",
    "server = 'https://api.tidesandcurrents.noaa.gov/mdapi/prod/webapi/stations/'\n",
    "\n",
    "myurl = (server + '.json?type='+station_type+'&units='+units)\n",
    "\n",
    "urlResponse = requests.get(myurl)\n",
    "content=urlResponse.json()\n",
    "\n",
    "stations = content['stations']\n",
    "stations_df = pd.DataFrame(stations)\n",
    "\n",
    "stations_df = stations_df[['id','lat','lng']]\n",
    "station_id = ['8651370','8656483']\n",
    "\n",
    "stations_df = stations_df[stations_df['id'].isin(station_id)]\n",
    "stations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5efc3d-62de-4be1-a34d-76367b33ec45",
   "metadata": {},
   "source": [
    "**Use glob to loop through the .csv files and run the data through the TADC using subprocess with SDC.py.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8452c078-b947-447d-b2ac-0df89d4195a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the folder containing the CSV files and use glob to iterate through the files\n",
    "dirname = 'path\\\\to\\\\csvfiles\\\\'\n",
    "csv_path = dirname + '*.csv'\n",
    "configpath = 'path\\\\to\\\\TADC_config_file_directory\\\\'\n",
    "\n",
    "i=0\n",
    "for fname in natsorted(glob.glob(csv_path)): # natsorted makes sure of the natural sorting of files\n",
    "    \n",
    "    file_name = os.path.split(fname)[-1]  # Extract the filename from the path\n",
    "    \n",
    "    with open(configpath + 'config.cfg', \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "        lines[22] = 'fname = ' + dirname + file_name + '\\n'\n",
    "        lines[47] = 'subordinate_lon = ' + str(stations_df.lng.iloc[i]) + '\\n'\n",
    "        lines[51] = 'subordinate_lat = ' + str(stations_df.lat.iloc[i]) + '\\n'\n",
    "    \n",
    "    with open(configpath + 'config.cfg', \"w\") as file:\n",
    "        file.writelines(lines)\n",
    "        \n",
    "    proc = subprocess.run(['python', configpath + 'SDC.py'], capture_output=True, text=True)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f4682cf-ac94-4091-84e4-0e81557c2de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the folder containing the CSV files and use glob to iterate through the files\n",
    "\n",
    "\n",
    "datastart = \"Data Start:  2018-09-01 00:00:00\"\n",
    "folder_path = dirname + '*.out'\n",
    "\n",
    "fulldata = []\n",
    "\n",
    "for fname in natsorted(glob.glob(folder_path)): # natsorted makes sure of the natural sorting of files\n",
    "\n",
    "    file_name = os.path.split(fname)[-1]  # Extract the filename from the path\n",
    "    out_path = dirname + file_name\n",
    "\n",
    "    with open(out_path, \"r\") as file:\n",
    "\n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            if datastart in line:\n",
    "                fulldata.append(int(file_name[0:7])) # these are the points that included a full 19 years of data\n",
    "                break  # Stop after finding the first match\n",
    "\n",
    "# for i in range(len(fulldata)):\n",
    "\n",
    "    # head_tail= os.path.split(str(fulldata[i]))\n",
    "    head_tail= os.path.split(fname)\n",
    "    \n",
    "    file_name = head_tail[1]  # Extract the filename from the path\n",
    "    out_path = dirname + file_name\n",
    "    # file_stem = Path(head_tail[1]).stem\n",
    "    \n",
    "    with open(out_path, \"r\") as file:\n",
    "        \n",
    "        lines = len(file.readlines())\n",
    "        \n",
    "    with open(out_path, \"r\") as file:\n",
    "        \n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            \n",
    "            if \"HWL\" in line:                   \n",
    "                    \n",
    "                colspecs = [(0,5),(7,15)]\n",
    "                df_fwf = pd.read_fwf(out_path, colspecs=colspecs,skiprows=line_number - 1, skipfooter=lines - line_number - 12, names=['Datums','meters'])\n",
    "                                \n",
    "                grid_fname = dirname + 'Datums_' + file_name + '.csv'\n",
    "                df_fwf.to_csv(grid_fname,index=False) # write the files to .csv\n",
    "\n",
    "                break  # Stop after finding the first match"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMAC_python",
   "language": "python",
   "name": "dmac_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
